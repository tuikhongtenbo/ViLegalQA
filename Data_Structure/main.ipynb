{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "mC751W1SRY5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec59e58-70b5-4ecc-c9b3-84fbc7495503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WckT9L-LtTLM"
      },
      "outputs": [],
      "source": [
        "# Main\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Check if the document is a \"Nghị định\"\n",
        "def is_nghi_dinh(document_data):\n",
        "    if \"name\" in document_data:\n",
        "        name_parts = document_data[\"name\"].split('-')\n",
        "        return len(name_parts) > 2 and name_parts[0].lower() == \"nghi\" and name_parts[1].lower() == \"dinh\"\n",
        "    return False\n",
        "\n",
        "# Clean text by removing excessive whitespace and newlines\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'(\\r\\n|\\n|\\s)+', ' ', text).strip()\n",
        "    return re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "# Clean and structure the extracted data\n",
        "def cleaned_structured_data(structured_data):\n",
        "    cleaned_data = structured_data.copy()\n",
        "    cleaned_data[\"Header\"] = clean_text(cleaned_data[\"Header\"])\n",
        "\n",
        "    for section in [\"Phần\", \"Chương\", \"Mục\", \"Tiểu Mục\", \"Điều\"]:\n",
        "        for item in cleaned_data[section]:\n",
        "            item[\"title\"] = clean_text(item[\"title\"])\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "# Extract structured data from the input JSON\n",
        "def extract_data(input_data):\n",
        "    result = {\n",
        "        \"Header\": \"\",\n",
        "        \"Phần\": [],\n",
        "        \"Chương\": [],\n",
        "        \"Mục\": [],\n",
        "        \"Tiểu Mục\": [],\n",
        "        \"Điều\": []\n",
        "    }\n",
        "\n",
        "    # Extract the header\n",
        "    header_pattern = re.compile(r\"^(.+?)(?=\\n\\n(Phần|Chương|Mục|Điều thứ|Điều)|$)\", re.DOTALL)\n",
        "    header_match = header_pattern.search(input_data[\"passage\"])\n",
        "    if header_match:\n",
        "        result[\"Header\"] = header_match.group(1).strip()\n",
        "\n",
        "    # Extract standalone \"Điều\"\n",
        "    dieu_pattern_alone = re.compile(r\"\\n\\n(?:Điều thứ|Điều)\\s+([0-9]+[abcd]?)[\\.:]?\\s+(.*?)(?=\\n\\n(Điều thứ|Điều|Mục|MỤC|Chương)|$)\", re.DOTALL)\n",
        "    dieu_matches = dieu_pattern_alone.finditer(input_data[\"passage\"])\n",
        "\n",
        "    for dieu_match in dieu_matches:\n",
        "        dieu_position = dieu_match.start()\n",
        "        preceding_text = input_data[\"passage\"][:dieu_position]\n",
        "        if \"\\n\\nChương\" not in preceding_text and \"\\n\\nMỤC\" not in preceding_text and \"\\n\\nPhần\" not in preceding_text:\n",
        "            dieu_number = dieu_match.group(1).strip()\n",
        "            dieu_title_content = dieu_match.group(2).strip()\n",
        "            result[\"Điều\"].append({\n",
        "                \"number\": dieu_number,\n",
        "                \"title\": dieu_title_content,\n",
        "                \"Chương\": \"\",\n",
        "                \"Mục\": \"\"\n",
        "            })\n",
        "\n",
        "    # Check for \"Phần\"\n",
        "    part_pattern = re.compile(r\"\\n\\nPhần\\s*([0-9IVXLCDM]+):?\\s*(.*?)(?=\\n\\n(Phần|Chương|Điều|Điều thứ)|$)\", re.DOTALL)\n",
        "    part_matches = list(part_pattern.finditer(input_data[\"passage\"]))\n",
        "\n",
        "    if part_matches:\n",
        "        for part_match in part_matches:\n",
        "            part_number = part_match.group(1).strip()\n",
        "            part_title = part_match.group(2).strip()\n",
        "            current_part = {\n",
        "                \"number\": part_number,\n",
        "                \"title\": part_title\n",
        "            }\n",
        "            result[\"Phần\"].append(current_part)\n",
        "\n",
        "            part_start = part_match.start()\n",
        "            part_end = part_matches[part_matches.index(part_match) + 1].start() if part_matches.index(part_match) + 1 < len(part_matches) else len(input_data[\"passage\"])\n",
        "\n",
        "            chapter_pattern = re.compile(r\"\\n\\n([0-9IVX]+)\\.\\s*(.*?)(?=\\n\\n[0-9]+\\.\\s*|$)\", re.DOTALL)\n",
        "            chapter_matches = list(chapter_pattern.finditer(input_data[\"passage\"][part_start:part_end]))\n",
        "\n",
        "            if chapter_matches:\n",
        "                for chapter_match in chapter_matches:\n",
        "                    chapter_number = chapter_match.group(1).strip()\n",
        "                    chapter_title = chapter_match.group(2).strip()\n",
        "                    current_chuong = {\n",
        "                        \"number\": chapter_number,\n",
        "                        \"title\": chapter_title,\n",
        "                        \"Phần\": part_number\n",
        "                    }\n",
        "                    result[\"Chương\"].append(current_chuong)\n",
        "\n",
        "                    chuong_start = chapter_match.start() + part_start\n",
        "                    chuong_end = chapter_matches[chapter_matches.index(chapter_match) + 1].start() + part_start if chapter_matches.index(chapter_match) + 1 < len(chapter_matches) else part_end\n",
        "\n",
        "                    extract_muc_and_dieu(result, input_data[\"passage\"][chuong_start:chuong_end], part_number, chapter_number)\n",
        "\n",
        "            else:\n",
        "                extract_dieu(result, input_data[\"passage\"][part_start:part_end], part_number)\n",
        "\n",
        "    else:\n",
        "        chapter_pattern = re.compile(r\"\\n\\nChương\\s*([0-9IVXLCDM]+):?\\s*(.*?)(?=\\n\\n(MỤC|Mục|Điều|Điều thứ|Chương)|$)\", re.DOTALL)\n",
        "        chapter_matches = list(chapter_pattern.finditer(input_data[\"passage\"]))\n",
        "\n",
        "        for chapter_match in chapter_matches:\n",
        "            chapter_number = chapter_match.group(1).strip()\n",
        "            chapter_title = chapter_match.group(2).strip()\n",
        "            current_chuong = {\n",
        "                \"number\": chapter_number,\n",
        "                \"title\": chapter_title,\n",
        "                \"Phần\": \"\"\n",
        "            }\n",
        "            result[\"Chương\"].append(current_chuong)\n",
        "\n",
        "            chuong_start = chapter_match.start()\n",
        "            chuong_end = chapter_matches[chapter_matches.index(chapter_match) + 1].start() if chapter_matches.index(chapter_match) + 1 < len(chapter_matches) else len(input_data[\"passage\"])\n",
        "\n",
        "            extract_muc_and_dieu(result, input_data[\"passage\"][chuong_start:chuong_end], \"\", chapter_number)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Extract \"Mục\" and \"Điều\"\n",
        "def extract_muc_and_dieu(result, text, part_number, chapter_number):\n",
        "    muc_pattern = re.compile(r\"\\n\\n(?:Mục|MỤC)\\s*([0-9]+)[\\.:]?\\s*(.*?)(?=\\n\\n(?:Điều|Điều thứ|Mục|Chương)|$)\", re.DOTALL)\n",
        "    muc_matches = list(muc_pattern.finditer(text))\n",
        "\n",
        "    if muc_matches:\n",
        "        for muc_match in muc_matches:\n",
        "            muc_number = muc_match.group(1).strip()\n",
        "            muc_title = muc_match.group(2).strip()\n",
        "            current_muc = {\n",
        "                \"number\": muc_number,\n",
        "                \"title\": muc_title,\n",
        "                \"Phần\": part_number,\n",
        "                \"Chương\": chapter_number\n",
        "            }\n",
        "            result[\"Mục\"].append(current_muc)\n",
        "\n",
        "            muc_start = muc_match.start()\n",
        "            muc_end = muc_matches[muc_matches.index(muc_match) + 1].start() if muc_matches.index(muc_match) + 1 < len(muc_matches) else len(text)\n",
        "\n",
        "            extract_dieu(result, text[muc_start:muc_end], part_number, chapter_number, muc_number)\n",
        "    else:\n",
        "        extract_dieu(result, text, part_number, chapter_number)\n",
        "\n",
        "# Extract \"Điều\"\n",
        "def extract_dieu(result, text, part_number, chapter_number=\"\", muc_number=\"\"):\n",
        "    dieu_pattern = re.compile(r\"\\n\\n(?:Điều thứ|Điều)\\s+([0-9]+[abcd]?)[\\.:]?\\s+(.*?)(?=\\n\\n(Điều thứ|Điều|Mục|MỤC|Chương)|$)\", re.DOTALL)\n",
        "    dieu_matches = dieu_pattern.finditer(text)\n",
        "\n",
        "    for dieu_match in dieu_matches:\n",
        "        dieu_number = dieu_match.group(1).strip()\n",
        "        dieu_title_content = dieu_match.group(2).strip()\n",
        "        result[\"Điều\"].append({\n",
        "            \"number\": dieu_number,\n",
        "            \"title\": dieu_title_content,\n",
        "            \"Phần\": part_number,\n",
        "            \"Chương\": chapter_number,\n",
        "            \"Mục\": muc_number\n",
        "        })\n",
        "\n",
        "# Paths for input and output directories\n",
        "input_folder = '/content/drive/MyDrive/contexts/'\n",
        "output_folder = '/content/drive/MyDrive/Structured_Nghị Định/'\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    input_file = os.path.join(input_folder, filename)\n",
        "    if os.path.isfile(input_file) and filename.endswith('.json'):\n",
        "        print(f\"Processing file: {input_file}\")\n",
        "\n",
        "        if os.path.getsize(input_file) > 10 * 1024 * 1024:  # Skip files larger than 10MB\n",
        "            print(f\"File {input_file} is too large to process.\")\n",
        "            continue\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            input_data = json.load(f)\n",
        "\n",
        "        if is_nghi_dinh(input_data):\n",
        "            structured_data = extract_data(input_data)\n",
        "            structured_data = cleaned_structured_data(structured_data)\n",
        "\n",
        "            output_file = os.path.join(output_folder, filename)\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(structured_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "            print(f\"Structured data saved to: {output_file}\")\n",
        "        else:\n",
        "            print(f\"Skipped non-Nghị định document: {input_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}